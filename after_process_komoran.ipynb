{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **앱스토어 리뷰 감성 분석**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제정의\n",
    "각 앱들의 리뷰들과 평점이 양의 관계에 있다고 가정하고 학습을 진행하여 평점을 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tsv 파일 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data_split(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        docs = [doc.lower().replace('\\n','').split('\\t') for doc in f.readlOOines()]\n",
    "        docs = [doc for doc in docs if len(doc) == 3]\n",
    "        \n",
    "    if not docs:\n",
    "        app_ids, texts, ratings = [], [], []\n",
    "\n",
    "    app_ids, texts, ratings = zip(*docs)\n",
    "    return list(app_ids), list(texts), list(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_app_ids, train_texts, train_ratings = get_data_split('./data/train_docs.txt')\n",
    "test_app_ids, test_texts, test_ratings = get_data_split('./data/test_docs.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "json 파일 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from konlpy.tag import Twitter, Komoran, Mecab\n",
    "from utils import Unknown_words, Post_ma, Make_dictionay\n",
    "import ujson\n",
    "\n",
    "twitter = Twitter()\n",
    "komoran = Komoran()\n",
    "mecab = Mecab()\n",
    "\n",
    "def get_data_json(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        texts = file.read()\n",
    "        data = ujson.loads(texts)\n",
    "        app_ids_list = list(data.keys())\n",
    "    return data, app_ids_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train_data, train_app_id_list = get_data_json('./data/train_json.txt')\n",
    "# test_data, test_app_id_list = get_data_json('./data/test_json.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 전처리: 형태소 분석\n",
    "1. 축약어, 오타:\n",
    "골치거리임, 전형적인 한국 사전에 없는 단어들도 많음, 사전에 없는 단어를 트위터사전에 넣어야 할 듯\n",
    "2. 띄어쓰기가 불분명함\n",
    "\n",
    "$\\Rightarrow$ data Cleaning class 필요성 있음\n",
    "\n",
    "### 데이터 후처리\n",
    "\n",
    "데이터 후처리란 형태소 분석기를 거친 후에 이상한 단어들, 신조어 등을 처리하는 작업\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 앱별로 공백문자를 나눠서 공통된 단어를 추출해보기? 앱별로 공통으로 많이 이야기 하는 단어들이 제대로 형태소 분석이 되는지 테스트 하기 위해 단어들을 추출해본다\n",
    "\n",
    "문제점: 일일이 확인 하기 어려움"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 형태소 분석 후처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## komoran\n",
    "이게 제일 나은듯?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_ma_docs, app_id_list = get_data_json('./data/ma_train_json_komoran.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'NA'확인 결과 문장 자체가 분석이 안된 것이 주로 ㅜㅜ 등 모음이나 자음이 따로 있는 경우 였다. 이것 부터 해결해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "UNK = Unknown_words(train_ma_docs, app_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown: 70342, total: 173157\n"
     ]
    }
   ],
   "source": [
    "unknown_list = UNK.get_unknown_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1021117809', (51, 11)]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UNK.unknown_loc['된다고하네요ㅜㅜ환불해주세요']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['이것', 'NP'],\n",
       " ['무료', 'NNG'],\n",
       " ['앱', 'NNP'],\n",
       " ['인줄', 'NNG'],\n",
       " ['알', 'VV'],\n",
       " ['았', 'EP'],\n",
       " ['는데', 'EC'],\n",
       " ['사람', 'NNG'],\n",
       " ['들', 'XSN'],\n",
       " ['이', 'JKS'],\n",
       " ['결제', 'NNG'],\n",
       " ['된다고하네요ㅜㅜ환불해주세요', 'NA']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ma_docs['1021117809'][51]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39287"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_rule, rest_unknown_list = UNK.remove_no_meanings(unknown_list, komoran)\n",
    "len(rest_unknown_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "리스트 형태로 바꿔줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "post_ma_list = UNK.shape_changer(unknown_rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "형태소 분석기 돌리기 전에 우선 문법을 좀 고쳐야 되는ㄱ ㅓ아닌가 ..됬는데 --> 됐는데...\n",
    "\n",
    "우선 단어 사전을 만들고 다시 unknown에 대해서 단어가 있으면 스페이싱을 하는 방법?\n",
    "\n",
    "혹은 자음모음 나눠서 단어장에 유사도가 비슷하면 채택?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Complete! total 31055 rules\n"
     ]
    }
   ],
   "source": [
    "post_ma = Post_ma(tokenizer_name='komoran')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "post_ma.update(post_ma_list, write=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['만원추가ㅡㅡ', 'NA']], [['만원', 'NNG'], ['추가', 'NNG']])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_ma.post_ma_pairs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70342"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(UNK.unknown_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_ma_docs = post_ma.replace_ma_docs(train_ma_docs, UNK.unknown_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "스위칭 됬는지 확인!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['이것', 'NP'],\n",
       " ['무료', 'NNG'],\n",
       " ['앱', 'NNP'],\n",
       " ['인줄', 'NNG'],\n",
       " ['알', 'VV'],\n",
       " ['았', 'EP'],\n",
       " ['는데', 'EC'],\n",
       " ['사람', 'NNG'],\n",
       " ['들', 'XSN'],\n",
       " ['이', 'JKS'],\n",
       " ['결제', 'NNG'],\n",
       " ['되', 'VV'],\n",
       " ['ㄴ다고', 'EC'],\n",
       " ['하', 'VX'],\n",
       " ['네요', 'EC'],\n",
       " ['환불', 'NNG'],\n",
       " ['하', 'XSV'],\n",
       " ['아', 'EC'],\n",
       " ['주', 'VX'],\n",
       " ['시', 'EP'],\n",
       " ['어요', 'EC']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ma_docs['1021117809'][51]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한번 다시 반복"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown: 42020, total: 133148\n"
     ]
    }
   ],
   "source": [
    "UNK = Unknown_words(train_ma_docs, app_id_list)\n",
    "unknown_list = UNK.get_unknown_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "새로 한번 적용했으나 이제 거의 여과가 안됨 아까는 70342 $\\rightarrow$ 39287 로 줄었는데, 지금은 42020 $\\rightarrow$ 39272 로 줄음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39272"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_rule, rest_unknown_list = UNK.remove_no_meanings(unknown_list, komoran)\n",
    "len(rest_unknown_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "post_ma_list = UNK.shape_changer(unknown_rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2748"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(post_ma_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 새로나온 룰을 추가시킨다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "post_ma.update(post_ma_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "post_ma.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33803"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(post_ma.post_ma_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "나머지는 단어 오탈자, 문장끝에 '닼ㅋㅋ' 이런 것들이 주로 많은 편이다, 이건 좀 나중에..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['팝업땨문에',\n",
       " '띵똥으로',\n",
       " '소맄ㅋ',\n",
       " '별로거든욯ㅎ~~^^',\n",
       " '간간히뒤에',\n",
       " '아둥바둥하는듭!',\n",
       " '받았는뎈ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ',\n",
       " '안되네요...쩝',\n",
       " '버벅거려..',\n",
       " '솔로탈춯함',\n",
       " '돈벋아먹을건',\n",
       " '구매내역삭제부탁드령용',\n",
       " '븅딱새끼들이.',\n",
       " '덜된건짛ㅎ',\n",
       " '바깟었는데',\n",
       " '딱이예욤!',\n",
       " '열어봐욯ㅎ',\n",
       " '인식됬으면',\n",
       " '페메에',\n",
       " '보정가능합니닼ㅋㅋ']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rest_unknown_list[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막 체크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown: 42020, total: 133148\n"
     ]
    }
   ],
   "source": [
    "UNK = Unknown_words(train_ma_docs, app_id_list)\n",
    "unknown_list = UNK.get_unknown_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_ma_as_json(filename, ma_docs):\n",
    "    with open(filename, 'w', encoding='utf-8') as out_file:\n",
    "        data = ujson.dumps(ma_docs, ensure_ascii=False)\n",
    "        print(data, file=out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_ma_as_json('./data/ma_train_json_komoran_after.txt', train_ma_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "트위터가 생각보다 별로 인듯..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_ma_docs, app_id_list = get_data_json('./data/ma_train_json_twitter.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 주요 품사별 추출 및 사전 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNG(일반 명사), NNP(고유 명사), VV(동사), VA(형용사)이며, 경우에 따라 MM(관형사), MAG(일반 부사), MAJ(접속 부사)\n",
    "\n",
    "일단 노 추출하고 사전 만들어보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from konlpy.tag import Twitter, Komoran, Mecab\n",
    "from utils import Unknown_words, Post_ma, Make_dictionay\n",
    "import ujson\n",
    "\n",
    "twitter = Twitter()\n",
    "komoran = Komoran()\n",
    "mecab = Mecab()\n",
    "\n",
    "def get_data_json(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        texts = file.read()\n",
    "        data = ujson.loads(texts)\n",
    "        app_ids_list = list(data.keys())\n",
    "    return data, app_ids_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_ma_docs, app_id_list = get_data_json('./data/ma_train_json_komoran_after.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data, _ = get_data_json('./data/train_json.txt')\n",
    "train_ratings = {k: v['ratings'] for k, v in train_data.items()}\n",
    "del train_data\n",
    "del _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "make_dict = Make_dictionay()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words: 100000\n",
      "complete!\n"
     ]
    }
   ],
   "source": [
    "make_dict.fit(train_ma_docs, maxwords=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_ma_docs_idx = make_dict.get_ma_docs_idx(train_ma_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['별', 'MM'],\n",
       " ['10', 'SN'],\n",
       " ['개', 'NNB'],\n",
       " ['드리', 'VV'],\n",
       " ['ㅂ니다', 'EC'],\n",
       " ['~', 'SO']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ma_docs[app_id_list[436]][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[394, 359, 154, 47, 34, 35]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ma_docs_idx[app_id_list[436]][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'드리/VV'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inverse dict: 편리하게 단어를 찾을 수 있음 \n",
    "make_dict.word_dict_idx_inverse[47]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IP부띠끄호텔에서/NA'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_dict.word_dict_idx_inverse[88888]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navie Bayesian Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한 documentation으로 보면, Input A은 문장내에 단어들의 조합이다 $[a_1, a_2 ,\\cdots, a_n]$. B는 rating점수다. 각 rating점수에 대해서, \n",
    "\n",
    "$P(B|A) = \\frac{P(A|B)P(B)}{P(A)}$가 성립하는데, $P(B)$는 사전 확률로 문서내에 각 rating이 나온 횟수로 측정 , $P(A|B)$는 Likelihood다.\n",
    "\n",
    "Chain rule를 활용하면 Likelihood $\\times$ prior 은 아래와 같이 된다.\n",
    "\n",
    "$P(a_1, a_2 ,\\cdots, a_n, B) \\\\ \n",
    "= P(a_1, a_2 ,\\cdots, a_n|B)P(B) \\\\\n",
    "= P(a_2, \\cdots, a_n|B, a_1)P(a_1|B)P(B) \\\\\n",
    "= \\cdots = P(a_n|B, a_1, \\cdots, a_{n-1}) \\cdots P(a_1|B)P(B)$\n",
    "\n",
    "여기서 $P(B)$를 제외한 부분은 구하기 어렵기 때문에 Navie가정을 하여($a_i$는 각기 다른 $a_i$과 독립이며, B에 대하여 조건부 독립이다) 계산. 즉,\n",
    "\n",
    "$P(a_1|B) = P(a_1|B)\\\\\n",
    "P(a_2|B, a_1) = P(a_2|B)\\\\\n",
    "\\quad \\vdots \\\\\n",
    "P(a_n|B, a_1, \\cdots, a_{n-1}) = P(a_n|B)$\n",
    "\n",
    "가 되는 것이고 종합하게 되면 $ P(a_1, a_2 ,\\cdots, a_n, B) = \\prod_{i=1}^{n}{P(a_i|B)}P(B)$ 가 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rating_counter = Counter()\n",
    "for app_id in app_id_list:\n",
    "    rating_counter\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### td-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec, Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Word2Vec()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
